{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction + Problem Definition\n",
        "In this notebook, 2 models will be built as solutions to a Semantic Natural Language Problem (NLP). The particular dataset that is being used is tweets about certain Airlines and it is the job of the models to identify whether they are positive or negative."
      ],
      "metadata": {
        "id": "mw_otDgsJVxS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xsQ5JdLzsrRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b5d9a08-78f6-498e-ad63-73d486653d01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "path = \"/content/gdrive/My Drive/DW_data/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preparation\n",
        "In the following code block, the data and labels will be converted into the approrpiate formats and will be prepared so that they can be used by a solution for the given NLP problem."
      ],
      "metadata": {
        "id": "xc8-H2OLw90E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(path+\"Tweets.csv\")\n",
        "df['airline_sentiment'] = df['airline_sentiment'].map({'positive': 0, 'negative': 1})\n",
        "\n",
        "#The below method is taken from https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n",
        "import re\n",
        "def remove_emojis(t):\n",
        "  regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\" \n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "  return regrex_pattern.sub(r'', t)\n",
        "\n",
        "# The punctuation marks that have allowed in the message is: # and @\n",
        "punc, numbs = '''!()-[]{};:'\"\\,<>./?$%^&*_~#@+''', '''0123456789'''\n",
        "results = []\n",
        "for i in range(len(df['text'])):\n",
        "  #Remove the emojis from the sentence.\n",
        "  temp = remove_emojis(df['text'][i])\n",
        "\n",
        "  #There is an edge case with ellipses, this will be handled here.\n",
        "  if \"...\" in temp:\n",
        "    temp = temp.replace(\"...\", \" \")\n",
        "\n",
        "  #Remove any unnecessary punctuations.\n",
        "  for elem in temp:\n",
        "    if '\"' == elem:\n",
        "      temp = temp.replace(elem, \"\")\n",
        "\n",
        "    if \"'\" == elem:\n",
        "      temp = temp.replace(elem, \"\")\n",
        "\n",
        "    if (\"http\" in elem) == False:\n",
        "      if elem in punc:\n",
        "        temp = temp.replace(elem, \"\")\n",
        "\n",
        "    #Is it best to remove this condition as some of the \n",
        "    if elem in numbs:\n",
        "      temp = temp.replace(elem, \"\")\n",
        "\n",
        "  #Store the sentences in the results array in the form of tokens.\n",
        "  if temp != \"\":\n",
        "    results.append(temp.lower())"
      ],
      "metadata": {
        "id": "8Jidjl8_0LRF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code block, the sentences will be converted into a 'dictionary' by splitting the sentences by spaces and then removing any duplicate tokens in the list."
      ],
      "metadata": {
        "id": "WPfb8jLQJcPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "sentences = []\n",
        "for entry in results:\n",
        "  tokens = nltk.word_tokenize(entry)\n",
        "\n",
        "  #Retrieve the stopwords from the English language and remove them from the tokens.\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "  tokens = [word for word in tokens if word not in stopwords]\n",
        "\n",
        "  #Reduce the tokens to the root form (lemmatize) of the word (token).\n",
        "  tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "  sentences.append(\" \".join(tokens))\n",
        "\n",
        "#Update the dataframe with the processed sentences.\n",
        "df['text'] = sentences"
      ],
      "metadata": {
        "id": "8lxsBtapJbss",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc9f07b3-fffe-46c3-c344-9e3d5873c508"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Segragation.\n",
        "The data will be split into 80%/0%/20% (train/validation/test). This test has been chosen due to the fact that the model will need the maximum amount of data that it can have. The reason why 20% of the data is used for testing is because we want the minimal amount of testing data that we can get away with to train the neural network appropriately."
      ],
      "metadata": {
        "id": "vzjH7xBQoDmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "x_train, x_test, y_train, y_test = train_test_split(df['text'], df['airline_sentiment'], test_size=0.2)\n",
        "x_train, x_test = np.array(x_train), np.array(x_test)\n",
        "y_train, y_test = np.array(y_train), np.array(y_test)"
      ],
      "metadata": {
        "id": "Yi6rHJAi3Rqf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Models.\n",
        "In the following code blocks, a LSTM and SVM model will be trained and tested on the provided dataset.\n",
        "\n",
        "Firstly, we'll need to build these models before we compare the performance of these models."
      ],
      "metadata": {
        "id": "ycyQZQVnz-ZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.engine.training import optimizers\n",
        "\n",
        "##### Recurrent Neural Network #####\n",
        "#Import the necessary libraries.\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding, SimpleRNN, Dropout, Dense, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "#Specify the vectorization of the sentences.\n",
        "n = 5000\n",
        "vectorize_layer = TextVectorization(max_tokens=n, output_mode='int')\n",
        "vectorize_layer.adapt(x_train)\n",
        "\n",
        "#Build the structure of the neural network.\n",
        "model = Sequential()\n",
        "model.add(vectorize_layer)\n",
        "model.add(Embedding(input_dim=n, output_dim=128, input_length=300))\n",
        "model.add(LSTM(units=32, return_sequences=True))\n",
        "model.add(LSTM(units=32))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "#Define the optimization, loss and metrics objects.\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss = keras.losses.BinaryCrossentropy()\n",
        "metrics = keras.metrics.BinaryAccuracy()\n",
        "\n",
        "#Compile, train and evaluate the model.\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=32, callbacks=[es])"
      ],
      "metadata": {
        "id": "LsV96PzO0H4E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb8f6993-e26f-4030-98db-4e32e616aac3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "289/289 [==============================] - 16s 37ms/step - loss: 0.3013 - binary_accuracy: 0.8795 - val_loss: 0.2052 - val_binary_accuracy: 0.9203\n",
            "Epoch 2/10\n",
            "289/289 [==============================] - 10s 35ms/step - loss: 0.1426 - binary_accuracy: 0.9451 - val_loss: 0.2235 - val_binary_accuracy: 0.9129\n",
            "Epoch 3/10\n",
            "289/289 [==============================] - 9s 32ms/step - loss: 0.1013 - binary_accuracy: 0.9618 - val_loss: 0.2460 - val_binary_accuracy: 0.9194\n",
            "Epoch 4/10\n",
            "289/289 [==============================] - 9s 31ms/step - loss: 0.0722 - binary_accuracy: 0.9755 - val_loss: 0.2668 - val_binary_accuracy: 0.9168\n",
            "Epoch 5/10\n",
            "289/289 [==============================] - 10s 35ms/step - loss: 0.0504 - binary_accuracy: 0.9829 - val_loss: 0.2921 - val_binary_accuracy: 0.9121\n",
            "Epoch 6/10\n",
            "289/289 [==============================] - 10s 36ms/step - loss: 0.0362 - binary_accuracy: 0.9869 - val_loss: 0.4265 - val_binary_accuracy: 0.9065\n",
            "Epoch 6: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code blocks, the Support Vector Machine will be implemented. However, the sentences will need to be converted into an appropriate numerical representation of the data before the model can be trained."
      ],
      "metadata": {
        "id": "N63AJDjzykqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
        "Tfidf_vect.fit(df['text'])\n",
        "\n",
        "x_train_vect = Tfidf_vect.transform(x_train)\n",
        "x_test_vect = Tfidf_vect.transform(x_test)"
      ],
      "metadata": {
        "id": "EWhKCFgD4Ssb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "SVM = svm.SVC(C=1.0, kernel='linear', degree=5, gamma='auto')\n",
        "SVM.fit(x_train_vect, y_train)\n",
        "\n",
        "print(\"SVM Accuracy Score -> \",accuracy_score(SVM.predict(x_test_vect), y_test)*100)"
      ],
      "metadata": {
        "id": "4p_2xzNsyqor",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "213c3356-fa74-41d1-ccc6-abbacb99c71c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy Score ->  92.20441749675184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compare the Models.\n",
        "Looking at the the accuracies of the models, it can be seen that the Support Vector Machine (SVM) is better at modeling the semantics of these tweets. This SVM is better than the Recurrent Neural Network (RNN) by a factor of 1.0172 (or 1.72%)."
      ],
      "metadata": {
        "id": "WoID8vK7BZsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnnResult = model.evaluate(x_test, y_test)[1]\n",
        "svmResult = accuracy_score(SVM.predict(x_test_vect), y_test)\n",
        "\n",
        "print(\"RNN Accuracy: \", round(rnnResult, 5) * 100)\n",
        "print(\"SVM Accuracy: \", round(svmResult, 5) * 100)\n",
        "\n",
        "print(\"Improvement:  \", round(round(svmResult, 5) / round(rnnResult, 5), 5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu_9-q1UBZR5",
        "outputId": "50c8fd01-3543-4c1b-a58f-682f0ea57884"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73/73 [==============================] - 0s 6ms/step - loss: 0.4265 - binary_accuracy: 0.9065\n",
            "RNN Accuracy:  90.645\n",
            "SVM Accuracy:  92.204\n",
            "Improvement:   1.0172\n"
          ]
        }
      ]
    }
  ]
}